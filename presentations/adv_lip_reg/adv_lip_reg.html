<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Adversarial Lipschitz Regularization</title>
<meta name="author" content="Dávid Terjék^{\dagger}"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://ammunk.com/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://ammunk.com/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="https://ammunk.com/assets/img/utils/extra.css"/>
<link rel="stylesheet" href="https://ammunk.com/reveal.js/plugin/highlight/zenburn.css"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://ammunk.com/assets/js/mathjax-config.js" defer> </script>
<script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script> $(function(){$("#includedContent").load("./bibtex.html");}); </script>
<meta name="description" content="Adversarial Lipschitz Regularization">
<script type="text/javascript" src=""></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<section id="sec-title-slide">
    <h1 class="title">Adversarial Lipschitz Regularization</h1><h2 class="author">By Dávid Terjék<sup>&dagger;</sup></h2>
    <h3 class="affiliation"><sup>&#8224;</sup>Robert Bosch Kft.</h3>
    <h2 class="presented">Presented By</h2>
    <h3 class="presented">Andreas Munk</h3>
    <h4 class="email"><a href="mailto:amunk@cs.ubc.ca">amunk@cs.ubc.ca</a></h4><h4 class="date">Nov 20, 2020</h4>
</section>

</section>
<section id="table-of-contents-section">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org7484e27">Penalization of Lipschitz constraint violation [<a href="#/slide-terjek2020adversarial">Terj&eacute;k, 2020</a>]</a></li>
<li><a href="#/slide-org3c6842e">Generative adversarial networks (GANS) [<a href="#/slide-goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>]</a></li>
<li><a href="#/slide-org382a48f">Wasserstein GAN [<a href="#/slide-arjovsky2017wasserstein">Arjovsky et&nbsp;al., 2017</a>]</a>
<ul>
<li><a href="#/slide-org57a76f6">Lipschitz Continuity</a></li>
</ul>
</li>
<li><a href="#/slide-org2b8691a">Improved Wasserstein GAN (WGAN-GP) [<a href="#/slide-gulrajani2017improved">Gulrajani et&nbsp;al., 2017</a>]</a>
<ul>
<li><a href="#/slide-org080d5b0">Lipschitz Continuity and Differentiability</a></li>
</ul>
</li>
<li><a href="#/slide-orgacc435d">Can we do better?</a></li>
<li><a href="#/slide-org9db4b5b">Adversarial Lipschitz regularization (ALR) [<a href="#/slide-terjek2020adversarial">Terj&eacute;k, 2020</a>]</a>
<ul>
<li><a href="#/slide-org1f312a6">\(^{*}\text{Assumptions}\)</a></li>
</ul>
</li>
<li><a href="#/slide-orgaaf64d8">WGAN-ALP</a></li>
<li><a href="#/slide-orgb40bfe2">Experiments</a>
<ul>
<li><a href="#/slide-orgb7ee0af">Experimental setup</a></li>
<li><a href="#/slide-org41d7c1c">CIFAR-10</a></li>
<li><a href="#/slide-org201d097">Comparison to other methods</a></li>
</ul>
</li>
<li><a href="#/slide-orgd28d4fc">Remaining issues</a></li>
<li><a href="#/slide-orgcdc9ee6">References</a></li>
</ul>
</div>
</div>
</section>
<div id="hiddenMathbox" style="display: none;">
<p>
\(
\newcommand{\ie}{i.e.}
\newcommand{\eg}{e.g.}
\newcommand{\etal}{\textit{et~al.}}
\newcommand{\wrt}{w.r.t.}
\newcommand{\bra}[1]{\langle #1 \mid}
\newcommand{\ket}[1]{\mid #1\rangle}
\newcommand{\braket}[2]{\langle #1 \mid #2 \rangle}
\newcommand{\bigbra}[1]{\big\langle #1 \big\mid}
\newcommand{\bigket}[1]{\big\mid #1 \big\rangle}
\newcommand{\bigbraket}[2]{\big\langle #1 \big\mid #2 \big\rangle}
\newcommand{\grad}{\boldsymbol{\nabla}}
\newcommand{\divop}{\grad\scap}
\newcommand{\pp}{\partial}
\newcommand{\ppsqr}{\partial^2}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\trans}[1]{#1^\mr{T}}
\newcommand{\dm}{\,\mathrm{d}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\krondel}[1]{\delta_{#1}}
\newcommand{\limit}[2]{\mathop{\longrightarrow}_{#1 \rightarrow #2}}
\newcommand{\measure}{\mathbb{P}}
\newcommand{\scap}{\!\cdot\!}
\newcommand{\intd}[1]{\int\!\dm#1\: }
\newcommand{\ave}[1]{\left\langle #1 \right\rangle}
\newcommand{\br}[1]{\left\lbrack #1 \right\rbrack}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\tub}[1]{\left\{#1\right\}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\evalat}[1]{\left.#1\right\vert}
\newcommand*{\given}{\mid}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\newcommand{\newterm}[1]{{\bf #1}}
\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\boldsymbol{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}
\def\eps{{\epsilon}}
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}
\def\vzero{{\boldsymbol{0}}}
\def\vone{{\boldsymbol{1}}}
\def\vmu{{\boldsymbol{\mu}}}
\def\vtheta{{\boldsymbol{\theta}}}
\def\va{{\boldsymbol{a}}}
\def\vb{{\boldsymbol{b}}}
\def\vc{{\boldsymbol{c}}}
\def\vd{{\boldsymbol{d}}}
\def\ve{{\boldsymbol{e}}}
\def\vf{{\boldsymbol{f}}}
\def\vg{{\boldsymbol{g}}}
\def\vh{{\boldsymbol{h}}}
\def\vi{{\boldsymbol{i}}}
\def\vj{{\boldsymbol{j}}}
\def\vk{{\boldsymbol{k}}}
\def\vl{{\boldsymbol{l}}}
\def\vm{{\boldsymbol{m}}}
\def\vn{{\boldsymbol{n}}}
\def\vo{{\boldsymbol{o}}}
\def\vp{{\boldsymbol{p}}}
\def\vq{{\boldsymbol{q}}}
\def\vr{{\boldsymbol{r}}}
\def\vs{{\boldsymbol{s}}}
\def\vt{{\boldsymbol{t}}}
\def\vu{{\boldsymbol{u}}}
\def\vv{{\boldsymbol{v}}}
\def\vw{{\boldsymbol{w}}}
\def\vx{{\boldsymbol{x}}}
\def\vy{{\boldsymbol{y}}}
\def\vz{{\boldsymbol{z}}}
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}
\def\mA{{\boldsymbol{A}}}
\def\mB{{\boldsymbol{B}}}
\def\mC{{\boldsymbol{C}}}
\def\mD{{\boldsymbol{D}}}
\def\mE{{\boldsymbol{E}}}
\def\mF{{\boldsymbol{F}}}
\def\mG{{\boldsymbol{G}}}
\def\mH{{\boldsymbol{H}}}
\def\mI{{\boldsymbol{I}}}
\def\mJ{{\boldsymbol{J}}}
\def\mK{{\boldsymbol{K}}}
\def\mL{{\boldsymbol{L}}}
\def\mM{{\boldsymbol{M}}}
\def\mN{{\boldsymbol{N}}}
\def\mO{{\boldsymbol{O}}}
\def\mP{{\boldsymbol{P}}}
\def\mQ{{\boldsymbol{Q}}}
\def\mR{{\boldsymbol{R}}}
\def\mS{{\boldsymbol{S}}}
\def\mT{{\boldsymbol{T}}}
\def\mU{{\boldsymbol{U}}}
\def\mV{{\boldsymbol{V}}}
\def\mW{{\boldsymbol{W}}}
\def\mX{{\boldsymbol{X}}}
\def\mY{{\boldsymbol{Y}}}
\def\mZ{{\boldsymbol{Z}}}
\def\mBeta{{\boldsymbol{\beta}}}
\def\mPhi{{\boldsymbol{\Phi}}}
\def\mLambda{{\boldsymbol{\Lambda}}}
\def\mSigma{{\boldsymbol{\Sigma}}}
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}
\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}
\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}
\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\newcommand{\vxlat}{\vx_{\mr{lat}}}
\newcommand{\vxobs}{\vx_{\mr{obs}}}
\newcommand{\block}[1]{\underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}}
\newcommand{\blockt}[1]{\begin{rcases} \begin{matrix} ~\\ ~\\ ~ \end{matrix} \end{rcases}{#1}}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\)
</p>
</div>
<section>
<section id="slide-org7484e27">
<h2 id="org7484e27">Penalization of Lipschitz constraint violation [<a href="#terjek2020adversarial">Terj&eacute;k, 2020</a>]</h2>
<ul>
<li>Required for Wasserstein Adversarial Networks [<a href="#arjovsky2017wasserstein">Arjovsky et&nbsp;al., 2017</a>]</li>
<li>Explicit regularization was previously thought infeasible [<a href="#petzka2018regularization">Petzka et&nbsp;al., 2018</a>]</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org3c6842e">
<h2 id="org3c6842e">Generative adversarial networks (GANS) [<a href="#goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>]</h2>
<ul>
<li>A generator is a neural network \(g:\mZ\rightarrow\mX\subseteq\real^{D_{x}}\) which maps from a random vector \(\vz\in\mZ\) to an output \(\vx \in \mX\)
<ul>
<li>\(g\) characterizes a distribution \(p_{\theta}(\vx)\), where \(\theta\in\Theta\subseteq\mathbb{R}^{D_g}\) denotes the generator&rsquo;s parameters</li>

</ul></li>
<li>Aim is to match \(p_{\theta}\) with some real target distribution \(p_{\mr{real}}\)</li>
<li>A critic/discriminator \(f_\phi:\mX\rightarrow \mF\) with parameters \(\phi\in\Phi\subseteq\real^{D_f}\)</li>

<li><p>
Generally we frame GANs as a minimax game,
</p>

<p>
\[\min_{\theta}\max_{\phi}h(p_{\theta},f_\phi)\]
</p></li>

<li>Originally Goodfellow et al. (2014) defined</li>

</ul>

<p>
\[ h(p_{\theta},f_\phi) = \E_{x\sim p_\mr{true}}\br{\log f_\phi(x)} + \E_{x\sim p_{\theta}}\br{\log (1 - f_\phi(x))}\]
</p>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org382a48f">
<h2 id="org382a48f">Wasserstein GAN [<a href="#arjovsky2017wasserstein">Arjovsky et&nbsp;al., 2017</a>]</h2>
<ul>
<li>Wasserstein Distance \(W_{p}(P_{1},P_{2})=\paren{\inf_{\gamma\in\Gamma(P_{1},P_{2})}\E_{\gamma}\br{d(x_{1},x_{2})^{p}}}^{\frac{1}{p}}\)</li>
<li>Kantorovich-Rubinstein duality [<a href="#villani2008optimal">Villani, 2008</a>],
\[ W_{1}(P_{1},P_{2}) = \sup_{\norm{f}_{\mr{L}}\leq1}\paren{\E_{P_{1}}\br{f} - \E_{P_{2}}\br{f}} \]</li>
<li>WGAN:
\[ \min_{\theta}\max_{\phi}\E_{p_\mr{true}}\br{f_{\phi}} - \E_{p_{\theta}}\br{f_\phi}, \quad \mr{s.t.}~\norm{f_{\phi}}_{\mr{L}}\leq 1\]</li>

<li>Notice the Lipschitz constraint \(\norm{f_{\phi}}_{\mr{Lip}}\leq 1\)</li>
<li>Constraint imposed using weight clipping \(\Phi\subseteq \mW = \br{-w,w}^{D_{f}}\), where \(w\) is some constant
<ul>
<li>Ensures global\(^{*}\) Lipschitz continuity with some unknown Lipschitz constant \(L\geq 0\)
<ul>
<li>\(^{*}\text{assuming}\) for all \(\phi\in\mW,~f_{\phi}\) is Lipschitz continuous or \(\mX\) is compact</li>
<li>Generally consider \(LW_{1}(P_{1},P_{2})\)</li>

</ul></li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
<section id="slide-org57a76f6">
<h3 id="org57a76f6">Lipschitz Continuity</h3>
<ul>
<li><p>
Definition: Given two metric spaces \(\paren{\mX, d_{\mX}}\) and \(\paren{\mY,
  d_{\mY}}\), a function \(f:\mX\rightarrow \mY\) is called Lipschitz continuous is
there exists a constant \(L\geq 0\) such that
</p>

<p>
\[\forall (x,x') \in\mX\times\mX, d_{\mY}(f(x),f(x')) \leq L d_{\mX}(x,x') \]
</p>

<p>
In particular consider \(\mX\subseteq\real^{D_{x}}\) and \(\mY\subseteq\real^{D_{y}}\) and the Euclidean distance, then
</p>

<p>
\[\forall (x,x') \in\mX\times\mX, \norm{f(x)-f(x')}_{2} \leq L \norm{x-x'}_{2}\]
</p></li>

<li>For more information on Lipschitz continuity and deep neural networks, see e.g. [<a href="#virmaux2018lipschitz">Virmaux and Scaman, 2018</a>].</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org2b8691a">
<h2 id="org2b8691a">Improved Wasserstein GAN (WGAN-GP) [<a href="#gulrajani2017improved">Gulrajani et&nbsp;al., 2017</a>]</h2>
<ul>
<li>Weight clipping reduces the function space</li>
<li>Soft regularization by considering the Lipschitz constraint with respect to the optimal coupling \(\gamma^{*}\)
<ul>
<li>\(\gamma^{*}\) is unknown</li>
<li>Use interpolated samples \(\tilde{x}\sim p_{i}\) between \(x_{\mr{real}}\) and \(x_{\mr{fake}}\)</li>
<li>Sample \(x_{\mr{real}}\sim p_{\mr{real}}\), \(x_{\mr{fake}}\sim p_{\theta}\), and do a random interpolation</li>

</ul></li>
<li>New objective using gradient penalties (GP)
\[ \min_{\theta}\max_{\phi}\E_{p_\mr{true}}\br{f_{\phi}} - \E_{p_{\theta}}\br{f_\phi} + \lambda\E_{\tilde{x}\sim p_{i}}\br{\paren{\norm{\grad_{\tilde{x}}f_{\phi}(\tilde{x})}_{2} - 1}^{2}} \]</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
<section id="slide-org080d5b0">
<h3 id="org080d5b0">Lipschitz Continuity and Differentiability</h3>
<ul>
<li><p>
<i>Using Theorem 3.1.6 in [<a href="#federer2014geometric">Federer, 1969</a>], we have that if \(f:\real^{n}\rightarrow\real^{m}\) is locally Lipschitz continuous function, then \(f\) is differentiable almost everywhere (except for a set of Lebesque measure zero). If \(f\) is Lipschitz continuous, then</i>
</p>

<p>
\[L(f)=\sup_{x\in\real^{n}}\norm{D_{x}f}_{op},\]
</p>

<p>
<i>where \(\norm{\mX}_{op}=\sup_{\vv:\norm{\vv}=1}\norm{\mX\vv}_{2}\) is the operator norm of \(\mX\in\real^{n\times m}\)</i>
</p></li>
<li><p>
In particular if \(m=1\), then \(D_{x}f = \trans{(\grad_{x}f)}\) and using Cauchy-Schwartz inequality we have,
</p>

<p>
\[\norm{\trans{(\grad_{x}f)}}_{op}=\sup_{\vv:\norm{\vv}=1}\abs{\langle\grad_{x}f,\vv\rangle}\leq\sup_{\vv:\norm{\vv}=1}\norm{\grad_{x}f}\norm{\vv}=\norm{\grad_{x}f}\]
</p></li>
<li><p>
Choose \(\vv=\frac{\grad_{x}f}{\norm{\grad_{x}f}}\) such that
</p>

<p>
\[L(f) = \sup_{x\in\real^{n}}\norm{\grad_{x}f}_{2}\]
</p></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
<section>
<section id="slide-orgacc435d">
<h2 id="orgacc435d">Can we do better?</h2>
<ul>
<li>Issues with WGAN-GP:
<ol>
<li>\(p_{i}\) is generally not equal to \(\gamma^{*}\), even when \(p_{g}=p_{\mr{true}}\). \(p_{i}\) is constructed using random interpolations, and important correlations may be unaccounted for</li>
<li>Too strong regularization. The gradient penalty term is fully minimized when \(\grad_{x}f(x) = 1\) for all \(x\)</li>

</ol></li>
<li><p>
[<a href="#petzka2018regularization">Petzka et&nbsp;al., 2018</a>] addresses point (2), by instead using the regularizer
\[\lambda\E_{p_{\tau}}\br{\max(0,\norm{\grad_{x}f(x)}_{2}-1)^{2}},\]
</p>

<p>
where \(p_{\tau}\) is another random pertubation distribution similar to \(p_{i}\).
</p>
<ul>
<li>Only penalizes gradient norms bigger than one</li>
<li>\(p_{\tau}\) did not provide improvements compared to \(p_{i}\)</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
<section>
<section id="slide-org9db4b5b">
<h2 id="org9db4b5b">Adversarial Lipschitz regularization (ALR) [<a href="#terjek2020adversarial">Terj&eacute;k, 2020</a>]</h2>
<ul>
<li>Previous methods impose soft Lipschitz constraints in the form of (estimated) expectations
<ul>
<li>Low probability of finding biggest violation of the Lipschitz constraint.</li>

</ul></li>
<li><p>
Consider the definition of Lipschitz continuity such that,
\[\norm{f}_{L}=L=\sup_{x,x'\in\mX}\frac{d_{\mY}(f(x),f(x'))}{d_{\mX}(x,x')}=\sup_{x,x+r\in\mX}\frac{d_{\mY}(f(x),f(x+r))}{d_{\mX}(x,x+r)},\]
</p>

<p>
where let \(x'=x+r\) and the supremum is with respect to both \(x\) and \(r\in\real^{n}\). To (approximately) ensure \(x+r\in\mX\), the Euclidean norm of \(r\) is bounded, \(\norm{r}_{2}\leq R\) for some \(R \gt 0\)
</p></li>
<li>Assuming that for each \(x\in\mX\) the supremum is attained, we can substitute \(\sup\rightarrow\max\), and consider
\[r'=\argmax_{x+r\in\mX}\frac{d_{\mY}(f(x),f(x+r))}{d_{\mX}(x,x+r)}\]</li>

</ul>

<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
<section>

<ul>
<li><p>
Under certain assumptions\(^{*}\) we can cast this problem into the following form
</p>

<p>
\[r'=\argmax_{\norm{r}_{2}\leq R}d_{\mY}(f(x),f(x+r))\]
</p></li>

<li><p>
The Adversarial Lipschitz Penalty (ALP),
</p>

<p>
\[ \mL_{\mr{ALP}} = \max\paren{0,\frac{d_{\mY}(f(x),f(x+r'))}{d_{\mX}(x,x+r')} - K}\]
</p>
<ul>
<li>Also considered the two-sided penalty (without \(\max(\cdot,\cdot)\)), but was found less stable.</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
<section id="slide-org1f312a6">
<h3 id="org1f312a6">\(^{*}\text{Assumptions}\)</h3>
<ul>
<li>Consider the optimization problem
\[ r'=\argmax_{x+r\in\mX, \norm{r}_{2}\leq R}\frac{d_{\mY}(f(x),f(x+r))}{d_{\mX}(x,x+r)} \]</li>
<li>For an arbitrary inequality constraint, assume that:
<ul>
<li>\(r'\) must lie somewhere on the constraint boundary</li>
<li>\(d_{\mX}(x,x+r)\) is constant on the constraint boundary and takes on some value \(c\)</li>
<li>Assume further that \(d_{\mX}(x,x+r)\leq c\) for all \(r\) within the feasible region</li>

</ul></li>
<li>If those assumptions are satisfied, we may ignore \(d_{\mX}(x,x+r)\)</li>
<li>In our case we have:
<ul>
<li>The constraint is written in terms of the 2-norm</li>
<li>\(d_{\mX}(x,x+r)\) is induced by the 2-norm, i.e. \(d_{\mX}(x,x+r)=\norm{x-(x+r)}_{2}\).</li>
<li>We may therefore ignore the denominator in the optimization problem</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
<section>
<section id="slide-orgaaf64d8">
<h2 id="orgaaf64d8">WGAN-ALP</h2>
<ul>
<li>Following [<a href="#miyato2019virtual">Miyato et&nbsp;al., 2019</a>] one can for every \(x\) find an approximation \(\epsilon r_{k}\approx r'\), where \(\epsilon\sim p_{\epsilon}\) and
\[r_{i+1}=\vt{\frac{\grad_{r}d_{\mY}(f(x),f(x+r))}{\norm{\grad_{r}d_{\mY}(f(x),f(x+r))}_{2}}}_{r=\xi r_{i}}\]
<ul>
<li>\(r_{0}\) is a random unit vector, and \(k=1\) is empirically found to be sufficient</li>

</ul></li>
<li><p>
Add ALP to encourage Lipschitz continuity for WGAN,
</p>

<p>
\[ h(\theta,\phi)=\E_{p_\mr{true}}\br{f_{\phi}} - \E_{p_{\theta}}\br{f_\phi} + \lambda\E_{x\sim p_{\mr{r,g}}}\br{\mL_{\mr{ALP}}(\phi,x)},\]
where \(p_{\mr{r,g}}\) is some combination of \(p_{\mr{true}}\) and \(p_{\theta}\) and
\[\mL_{\mr{ALP}}(\phi,x)=\max\paren{0,\frac{d_{\mY}(f_{\phi}(x),f_{\phi}(x+r'))}{d_{\mX}(x,x+r')} - K},\]
</p>

<p>
with \(d_{\mY}(f(x),f(x+r'))=\abs{f(x)-f(x+r')}\) and \(d_{\mX}(x,x+r')=\norm{x-(x+r')}_{2}=\norm{r'}_{2}\)
</p></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
<section>
<section id="slide-orgb40bfe2">
<h2 id="orgb40bfe2">Experiments</h2>
<div class="outline-text-2" id="text-orgb40bfe2">
</div>
</section>
<section id="slide-orgb7ee0af">
<h3 id="orgb7ee0af">Experimental setup</h3>
<ul>
<li>Similar experimental setup as for WGAN-GP</li>
<li>Regularizer strength: \(\lambda=100\)</li>
<li>Lipschitz constant: \(K=1\)</li>
<li>For finding \(r'\): \(\xi=10\), \(p_{\epsilon}=\mU(0.1,10)\)</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
<section id="slide-org41d7c1c">
<h3 id="org41d7c1c">CIFAR-10</h3>

<div id="orgb271088" class="figure">
<p><img src="../../../assets/img/adv_lip_reg/figs/cifar10_images.png" alt="cifar10_images.png" width="1400" />
</p>
<p><span class="figure-number">Figure 1: </span>Random samples using WGAN-ALP with (a) and without (b) batch normalization (BN).</p>
</div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
<section id="slide-org201d097">
<h3 id="org201d097">Comparison to other methods</h3>

<div id="org90fde5b" class="figure">
<p><img src="../../../assets/img/adv_lip_reg/figs/cifar10_table.png" alt="cifar10_table.png" width="1400" />
</p>
</div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
<section>
<section id="slide-orgd28d4fc">
<h2 id="orgd28d4fc">Remaining issues</h2>
<ul>
<li>Still have an expectation to enforce Lipschitz continuity
\[\E_{x\sim p_{\mr{r,g}}}\br{L_{\mr{ALP}}(\phi,x)}=\E_{x\sim p_{\mr{r,g}}}\br{\max\paren{0,\frac{d_{\mY}(f_{\phi}(x),f_{\phi}(x+r'))}{d_{\mX}(x,x+r')} - K}}\]
<ul>
<li>How to do away with that as well?</li>

</ul></li>
<li>The upper bound on \(r\) to ensure \(x+r\in\mX\), which the ability to find the maximum violation of the Lipschitz constraint</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
<section>
<section id="slide-orgcdc9ee6">
<h2 id="orgcdc9ee6">References</h2>
<div id="includedContent"></div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
</div>
</div>
<script src="https://ammunk.com/reveal.js/dist/reveal.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/highlight/highlight.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/search/search.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/zoom/zoom.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/notes/notes.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: 't',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,
width: 2000,
height: 1400,
margin: 0.00,
minScale: 0.30,
maxScale: 2.50,

transition: 'linear',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealHighlight, RevealSearch, RevealZoom, RevealNotes ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
