<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Assisting the Adversary to Improve GAN Training</title>
<meta name="author" content="Andreas Munk, William Harvey & Frank Wood"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://ammunk.com/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://ammunk.com/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="https://ammunk.com/assets/img/utils/extra.css"/>
<link rel="stylesheet" href="https://ammunk.com/reveal.js/plugin/highlight/zenburn.css"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://ammunk.com/assets/js/mathjax-config.js" defer> </script>
<script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script> $(function(){$("#includedContent").load("./bibtex.html");}); </script>
<meta name="description" content="Assisting the Adversary to Improve GAN Training">
<script type="text/javascript" src=""></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<section id="sec-title-slide">
    <h1 class="title">Assisting the Adversary to Improve GAN Training</h1><h2 class="author">Andreas Munk, William Harvey &amp; Frank Wood</h2><h4 class="email"><a href="mailto:amunk@cs.ubc.ca">amunk@cs.ubc.ca</a></h4><h4 class="date">March 26, 2021</h4>
</section>

</section>
<section id="table-of-contents-section">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org73ca8f0">Closing the gap between theory and practice</a></li>
<li><a href="#/slide-orge5087cc">Generative adversarial networks (GANs) [<a href="#/slide-goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>]</a></li>
<li><a href="#/slide-orge9f7bdb">Why GANs?</a></li>
<li><a href="#/slide-orga3db09c"><i>The optimal adversary assumption</i> and the minimization of divergences</a></li>
<li><a href="#/slide-org4f69146">Does an optimal adversary lead to optimal gradients?</a></li>
<li><a href="#/slide-org454b5eb">Adversary constructors</a></li>
<li><a href="#/slide-orgdcfdac8">Assisting the Adversary - introducing AdvAs</a></li>
<li><a href="#/slide-org52196cb">Removing the hyperparameter \(\lambda\)</a></li>
<li><a href="#/slide-orgb5ee745">Experiments</a></li>
<li><a href="#/slide-orgcbeac63">MNIST and WGAN-GP [<a href="#/slide-gulrajani2017improved">Gulrajani et&nbsp;al., 2017</a>]</a></li>
<li><a href="#/slide-org4f7cdc1">CelebA and StyleGan [<a href="#/slide-karras2020analyzing">Karras et&nbsp;al., 2020</a>]</a></li>
<li><a href="#/slide-org0a2406b">References</a></li>
</ul>
</div>
</div>
</section>
<div id="hiddenMathbox" style="display: none;">
<p>
\(
\newcommand{\ie}{i.e.}
\newcommand{\eg}{e.g.}
\newcommand{\etal}{\textit{et~al.}}
\newcommand{\wrt}{w.r.t.}
\newcommand{\bra}[1]{\langle #1 \mid}
\newcommand{\ket}[1]{\mid #1\rangle}
\newcommand{\braket}[2]{\langle #1 \mid #2 \rangle}
\newcommand{\bigbra}[1]{\big\langle #1 \big\mid}
\newcommand{\bigket}[1]{\big\mid #1 \big\rangle}
\newcommand{\bigbraket}[2]{\big\langle #1 \big\mid #2 \big\rangle}
\newcommand{\grad}{\boldsymbol{\nabla}}
\newcommand{\divop}{\grad\scap}
\newcommand{\pp}{\partial}
\newcommand{\ppsqr}{\partial^2}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\trans}[1]{#1^\mr{T}}
\newcommand{\dm}{\,\mathrm{d}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\krondel}[1]{\delta_{#1}}
\newcommand{\limit}[2]{\mathop{\longrightarrow}_{#1 \rightarrow #2}}
\newcommand{\measure}{\mathbb{P}}
\newcommand{\scap}{\!\cdot\!}
\newcommand{\intd}[1]{\int\!\dm#1\: }
\newcommand{\ave}[1]{\left\langle #1 \right\rangle}
\newcommand{\br}[1]{\left\lbrack #1 \right\rbrack}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\tub}[1]{\left\{#1\right\}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\evalat}[1]{\left.#1\right\vert}
\newcommand*{\given}{\mid}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\newcommand{\newterm}[1]{{\bf #1}}
\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\boldsymbol{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}
\def\eps{{\epsilon}}
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}
\def\vzero{{\boldsymbol{0}}}
\def\vone{{\boldsymbol{1}}}
\def\vmu{{\boldsymbol{\mu}}}
\def\vtheta{{\boldsymbol{\theta}}}
\def\va{{\boldsymbol{a}}}
\def\vb{{\boldsymbol{b}}}
\def\vc{{\boldsymbol{c}}}
\def\vd{{\boldsymbol{d}}}
\def\ve{{\boldsymbol{e}}}
\def\vf{{\boldsymbol{f}}}
\def\vg{{\boldsymbol{g}}}
\def\vh{{\boldsymbol{h}}}
\def\vi{{\boldsymbol{i}}}
\def\vj{{\boldsymbol{j}}}
\def\vk{{\boldsymbol{k}}}
\def\vl{{\boldsymbol{l}}}
\def\vm{{\boldsymbol{m}}}
\def\vn{{\boldsymbol{n}}}
\def\vo{{\boldsymbol{o}}}
\def\vp{{\boldsymbol{p}}}
\def\vq{{\boldsymbol{q}}}
\def\vr{{\boldsymbol{r}}}
\def\vs{{\boldsymbol{s}}}
\def\vt{{\boldsymbol{t}}}
\def\vu{{\boldsymbol{u}}}
\def\vv{{\boldsymbol{v}}}
\def\vw{{\boldsymbol{w}}}
\def\vx{{\boldsymbol{x}}}
\def\vy{{\boldsymbol{y}}}
\def\vz{{\boldsymbol{z}}}
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}
\def\mA{{\boldsymbol{A}}}
\def\mB{{\boldsymbol{B}}}
\def\mC{{\boldsymbol{C}}}
\def\mD{{\boldsymbol{D}}}
\def\mE{{\boldsymbol{E}}}
\def\mF{{\boldsymbol{F}}}
\def\mG{{\boldsymbol{G}}}
\def\mH{{\boldsymbol{H}}}
\def\mI{{\boldsymbol{I}}}
\def\mJ{{\boldsymbol{J}}}
\def\mK{{\boldsymbol{K}}}
\def\mL{{\boldsymbol{L}}}
\def\mM{{\boldsymbol{M}}}
\def\mN{{\boldsymbol{N}}}
\def\mO{{\boldsymbol{O}}}
\def\mP{{\boldsymbol{P}}}
\def\mQ{{\boldsymbol{Q}}}
\def\mR{{\boldsymbol{R}}}
\def\mS{{\boldsymbol{S}}}
\def\mT{{\boldsymbol{T}}}
\def\mU{{\boldsymbol{U}}}
\def\mV{{\boldsymbol{V}}}
\def\mW{{\boldsymbol{W}}}
\def\mX{{\boldsymbol{X}}}
\def\mY{{\boldsymbol{Y}}}
\def\mZ{{\boldsymbol{Z}}}
\def\mBeta{{\boldsymbol{\beta}}}
\def\mPhi{{\boldsymbol{\Phi}}}
\def\mLambda{{\boldsymbol{\Lambda}}}
\def\mSigma{{\boldsymbol{\Sigma}}}
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\boldsymbol{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}
\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}
\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}
\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\newcommand{\vxlat}{\vx_{\mr{lat}}}
\newcommand{\vxobs}{\vx_{\mr{obs}}}
\newcommand{\block}[1]{\underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}}
\newcommand{\blockt}[1]{\begin{rcases} \begin{matrix} ~\\ ~\\ ~ \end{matrix} \end{rcases}{#1}}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\)
</p>
</div>

<section>
<section id="slide-org73ca8f0">
<h2 id="org73ca8f0">Closing the gap between theory and practice</h2>
<ul>
<li>The GAN framework is a minimax game, that pivots an adversary
(critic/discriminator) against a generator</li>

<li>The adversary is often assumed optimal in theoretical analysis and in the
construction of different GANs
<ul>
<li>In practice this is essentially never true</li>

</ul></li>

<li>We consider largely overlooked approach to constraining GAN training
[<a href="#mescheder2017numerics">Mescheder et&nbsp;al., 2017</a>,<a href="#nagarajan2017gradient">Nagarajan and Kolter, 2017</a>]
<ul>
<li>Re-motivated to bridge the gap between theory and practice</li>

</ul></li>

<li>Accounts for the adversary being suboptimal - <i>The Adversarys Assistant</i>
(AdvAs) [<a href="#munk2020assisting">Munk et&nbsp;al., 2020</a>]
<ul>
<li>A regularizer which encourages the generator to move towards points where
the current adversary is optimal</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orge5087cc">
<h2 id="orge5087cc">Generative adversarial networks (GANs) [<a href="#goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>]</h2>
<ul>
<li>A generator is a neural network \(g:\gZ\rightarrow\gX\subseteq\real^{D_{x}}\)
which maps from a random vector \(\vz\in\gZ\) to an output \(\vx \in \gX\)
<ul>
<li>\(g\) characterizes a distribution \(p_{\theta}(\vx)\), where
\(\theta\in\Theta\subseteq\mathbb{R}^{D_g}\) denotes the generator&rsquo;s
parameters</li>

</ul></li>

<li>Aim is to match \(p_{\theta}\) with some real target distribution
\(p_{\mr{true}}\)</li>

<li>An adversary \(a_\phi:\gX\rightarrow \gA\) with parameters
\(\phi\in\Phi\subseteq\real^{D_a}\)</li>

<li><p>
Generally we frame GANs as a minimax game,
</p>

<p>
\[\min_{\theta}\max_{\phi}h(p_{\theta},a_\phi)\]
</p></li>

<li>If the minimax game of perfectly solved, then \(p_{\theta} = p_{\mr{true}}\)</li>

<li><p>
Originally Goodfellow et al. (2014) defined \(\gA=\br{0,1}\)
</p>

<p>
\[ h(p_{\theta},a_\phi) = \mathbb{E}_{x\sim p_{\mr{true}}} \left[ \log a_\phi(x)
    \right] + \mathbb{E}_{x\sim p_{\theta}} \left[ \log (1- a_\phi(x)) \right].\]
</p></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orge9f7bdb">
<h2 id="orge9f7bdb">Why GANs?</h2>
<ul>
<li>Generative modeling using implicit densities</li>

<li>Produce photo-realistic images [<a href="#zhou2019hype">Zhou et&nbsp;al., 2019</a>]</li>

<li>However, they tend to have unstable training dynamics [<a href="#mescheder2018which">Mescheder et&nbsp;al., 2018</a>]
<ul>
<li>Primarily addressed using different GAN objectives or by using various
regularization techniques when training the adversary</li>

</ul></li>

<li>We focus on regularizing the training of the generator
<ul>
<li>Compatible with above-mentioned regularization techniques</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orga3db09c">
<h2 id="orga3db09c"><i>The optimal adversary assumption</i> and the minimization of divergences</h2>
<ul>
<li>Before each generator update, the adversary is assumed optimal
<ul>
<li>That is, if \(\gF\) is the class of permissible adversary functions, then for
a particular \(\theta\) the adversary is optimal if \(a^{*}=\argmax_{a \in \gF}h(p_{\theta},a)\)</li>

</ul></li>

<li><p>
When \(a\) is a parameterized by a neural net with parameters \(\phi\in\Phi\), we
define
</p>

<p>
\[\Phi^*(\theta) = \{\phi \in \Phi \mid h(p_\theta, a_\phi) = \max_{a \in
    \gF}h(p_{\theta},a) \}\]
</p></li>

<li><p>
This assumption turns the two-player minimax game into a case of minimizing an
objective w.r.t. \(\theta\) alone
</p>

<p>
\[
  \gM(p_{\theta}) = \max_{a \in \gF}h(p_{\theta},a) = h(p_\theta, a_{\phi^*}) \quad \text{where} \quad \phi^*\in\Phi^*(\theta).
  \]
</p></li>

<li>For instance [<a href="#goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>] showed that \(\gM(p_{\theta}) =
  2\cdot\text{JSD}(p_{\mr{true}} || p_\theta) - \log 4\)</li>

<li>Many other GAN approaches make use of the optimal adversary assumption to show
that \(\gM(p_{\theta})\) reduces to some divergence (e.g. WGAN
[<a href="#arjovsky2017wasserstein">Arjovsky et&nbsp;al., 2017</a>])</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org4f69146">
<h2 id="org4f69146">Does an optimal adversary lead to optimal gradients?</h2>
<ul>
<li>The goal of training the generator can be seen as minimizing \(\gM(p_{\theta})\)
<ul>
<li>In practice the generator update requires it&rsquo;s gradient \(\grad_\theta
    \gM(p_{\theta})\) (e.g. gradient descent)</li>

</ul></li>

<li>Assume we have generator parameters \(\theta'\). Consider
\(\phi^*\in\Phi^*(\theta')\) such that \(h (p_{\theta'}, a_{\phi^*})\) and
\(\gM(p_{\theta'})\) are equal in value
<ul>
<li>We want \(\grad_\theta \gM(p_{\theta})\mid_{\theta=\theta'}\) but calculate in
practice the partial derivative \(D_1 h (p_\theta,
    a_{\phi^*})\mid_{\theta=\theta'}\)
<ul>
<li>Where we defined \(D_1 h (p_\theta, a_\phi)\) to denote the partial
derivative of \(h(p_\theta, a_\phi)\) with respect to \(\theta\) with \(\phi\)
kept constant. Similarly, define \(D_2 h (p_\theta, a_\phi)\) to denote the
derivative of \(h (p_\theta, a_\phi)\) with respect to \(\phi\), with \(\theta\)
held constant.</li>

</ul></li>
<li>It is not immediately obvious these two quantities are equal</li>

</ul></li>

<li>By extending Theorem 1 in [<a href="#milgrom2002envelope">Milgrom and Segal, 2002</a>] we have that</li>

</ul>
<blockquote>
<p>
<b>Theorem 1</b>. Let \(\gM(p_{\theta})=h(p_{\theta},a_{\phi^*})\) for any
  \(\phi^*\in\Phi^{*}(\theta)\). Assuming that \(\gM(p_{\theta})\) is differentiable
  w.r.t. \(\theta\) and \(h(p_{\theta},a_{\phi})\) is differentiable w.r.t. \(\theta\)
  for all \(\phi\in\Phi^{*}(\theta)\), then if \(\phi^{*}\in\Phi^{*}(\theta)\) we
  have
  \[
  \grad_{\theta}\gM(p_{\theta})=D_1 h(p_{\theta},a_{\phi^{*}})
  \]
</p>
</blockquote>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org454b5eb">
<h2 id="org454b5eb">Adversary constructors</h2>
<ul>
<li><p>
An adversary constructor is a function \(f:\Theta\rightarrow\Phi\), where for
all \(\theta \in \Theta\), \(f(\theta) = \phi^*\in\Phi^{*}(\theta)\),
</p>

<p>
\[h(p_{\theta},a_{f(\theta)})=\max_{a\in\gF}h(p_{\theta},a)\]
</p></li>

<li><p>
We cannot compute the function \(f(\theta)\) in general. However, they
lead to a condition which must be satisfied for Theorem 1 to be invoked
</p>

<blockquote>
<p>
<b>Corollary 1</b>. Let \(f:\Theta\rightarrow\Phi\) be a differentiable mapping such
  that for all \(\theta \in \Theta\),
  \(\gM(p_{\theta})=h(p_{\theta},a_{f(\theta)})\). If the conditions in Theorem 1
  are satisfied and the Jacobian matrix of \(f\) with respect to \(\theta\),
  \(\mJ_{\theta}(f)\) exists for all \(\theta\in\Theta\) then
</p>
<div>
\begin{equation}\label{eq:cor}
  \trans{D_2 h(p_{\theta},a_{f(\theta)})} \mJ_{\theta}(f) = 0.
\end{equation}

</div>
</blockquote></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>


</section>
</section>
<section>
<section id="slide-orgdcfdac8">
<h2 id="orgdcfdac8">Assisting the Adversary - introducing AdvAs</h2>
<ul>
<li>E.q. \ref{eq:cor} is a necessary condition for invoking Theorem 1</li>
<li>\(\trans{D_2 h(p_{\theta},a_{\phi})} \mJ_{\theta}(f)\) could be viewed as a
measure of &ldquo;closeness&rdquo;.
<ul>
<li>Cannot be computed due to the infeasible Jacobian</li>

</ul></li>
<li>Can compute \(D_2 h(p_{\theta},a_{\phi})\)
<ul>
<li>Use the magnitude as an approximate measure of how &ldquo;close&rdquo; we are to
having an optimal adversary</li>

</ul></li>

<li><p>
Define the objective functions
</p>

<div>
\begin{align}
\grad_\theta L_{\text{gen}}(\theta, \phi) &= \grad_\theta h(p_{\theta},a_\phi) \nonumber \\
\grad_\phi L_{\text{adv}}(\theta, \phi) &= -\grad_\phi h(p_{\theta},a_\phi) \nonumber
\end{align}

</div></li>

<li><p>
Using AdvAs implies
</p>

<p>
\[
 L^{\mr{AdvAs}}_{\mr{gen}}(\theta,\phi) = L_{\mr{gen}}(\theta,\phi) + \lambda\cdot r(\theta,\phi)
 \]
</p>

<p>
with \(r(\theta,\phi) = \norm{\grad_\phi {L_{\mr{adv}}}(\theta,\phi)}_2^2\) and \(\lambda\) being a hyperparameter.
</p></li>

</ul>

<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
<section>


<div id="orgdbbc600" class="figure">
<p><img src="../../../assets/img/advas/figs/advas_alg.png" alt="advas_alg.png" width="1000" />
</p>
</div>

<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
<section>

<ul>
<li>Preserves convergence results:
<ol>
<li>Using AdvAs does not interfere with adversary updates</li>
<li>Assuming an optimal adversary we have
\(L^{\mr{AdvAs}}_{\mr{gen}}(\theta,\phi) \rightarrow L_{\text{gen}}(\theta,
     \phi)\)</li>

</ol></li>

<li>Use an unbiased estimate \(\tilde{r}(\theta, \phi) \approx \norm{\grad_\phi
  {L_{\mr{adv}}}(\theta,\phi)}_2^2\)
<ul>
<li>consider two independent and unbiased estimates of \(\grad_\phi
    {L_{\mr{adv}}}(\theta,\phi)\) denoted \(\mX,\mX'\). Then
\(\E\br{\trans{\mX}\mX'}=\trans{\E\br{\mX}}\E\br{\mX'}=\norm{\grad_{\phi}L_{\mr{adv}}(\theta,\phi)}_2^2\).</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org52196cb">
<h2 id="org52196cb">Removing the hyperparameter \(\lambda\)</h2>
<ul>
<li>If \(\lambda\) is too large it may destabilize training. If too small it is
equivalent to not using AdvAs.</li>

<li><p>
Define
</p>

<div>
\begin{align*}
\vg_{\mr{orig}}(\theta, \phi) &= \grad_{\theta}L_{\mr{gen}}(\theta,\phi) \\
\vg_{\mr{AdvAs}}(\theta, \phi) &= \grad_{\theta}\tilde{r}(\theta,\phi) \\
\vg_{\mr{total}}(\theta, \phi, \lambda) &= \grad_{\theta}L^{\mr{AdvAs}}_{\mr{gen}}(\theta,\phi) \\
                       &= \vg_{\mr{orig}}(\theta, \phi) + \lambda \vg_{\mr{AdvAs}}(\theta, \phi).
\end{align*}

</div></li>

<li>We set</li>

</ul>
<div>
\begin{equation}
  \lambda = \min \left( 1, \frac{\norm{\vg_{\mr{orig}}(\theta, \phi)}_2}{\norm{\vg_{\mr{AdvAs}}(\theta, \phi)}_2}\right) \nonumber
\end{equation}

</div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orgb5ee745">
<h2 id="orgb5ee745">Experiments</h2>

</section>
</section>
<section>
<section id="slide-orgcbeac63">
<h2 id="orgcbeac63">MNIST and WGAN-GP [<a href="#gulrajani2017improved">Gulrajani et&nbsp;al., 2017</a>]</h2>
<ul>
<li>The best FID score reached is improved by \(28\%\) when using AdvAs</li>

</ul>


<div id="orgdd95633" class="figure">
<p><object type="image/svg+xml" data="../../../assets/img/advas/figs/results_FID60000_plotting-v100mnist.svg" class="org-svg" width="1300">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 2: </span>FID scores throughout training for the WGAN-GP objective on MNIST, estimated using 60,000 samples from the generator. We plot up to a maximum of 40,000 iterations. When plotting against time (right), this means some lines end before the two hours we show. The blue line shows the results with AdvAs, while the others are baselines with different values of \(n_{\mr{adv}}\), the number of adversary updates per generator update.</p>
</div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>


</section>
</section>
<section>
<section id="slide-org4f7cdc1">
<h2 id="org4f7cdc1">CelebA and StyleGan [<a href="#karras2020analyzing">Karras et&nbsp;al., 2020</a>]</h2>

<div id="org9aa2ed4" class="figure">
<p><object type="image/svg+xml" data="../../../assets/img/advas/figs/graph-and-images.svg" class="org-svg" width="1000">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 3: </span><b>Bottom:</b> FID scores throughout training estimated with \(1000\) samples, plotted against number of epochs (left) and training time (right). FID scores for AdvAs decrease more on each iteration at the start of training and converge to be \(7.5\%\) lower. <b>Top:</b> The left two columns show uncurated samples with and without AdvAs after 2 epochs. The rightmost two columns show uncurated samples from networks at the end of training. In each grid of images, each row is generated by a network with a different training seed and shows 3 images generated by passing a different random vector through this network. AdvAs leads to obvious qualitative improvement early in training.</p>
</div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org0a2406b">
<h2 id="org0a2406b">References</h2>
<div id="includedContent"></div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
</div>
</div>
<script src="https://ammunk.com/reveal.js/dist/reveal.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/highlight/highlight.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/search/search.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/zoom/zoom.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/notes/notes.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: 't',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,
width: 2000,
height: 1400,
margin: 0.00,
minScale: 0.30,
maxScale: 2.50,

transition: 'linear',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealHighlight, RevealSearch, RevealZoom, RevealNotes ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
