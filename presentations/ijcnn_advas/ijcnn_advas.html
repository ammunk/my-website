<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Assisting the Adversary to Improve GAN Training</title>
<meta name="author" content="Andreas Munk, William Harvey & Frank Wood"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://ammunk.com/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://ammunk.com/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="https://ammunk.com/assets/img/utils/extra.css"/>

<link rel="stylesheet" href="https://ammunk.com/assets/img/utils/logo_ubc_plai.css"/>
<link rel="stylesheet" href="https://ammunk.com/reveal.js/plugin/highlight/zenburn.css"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://ammunk.com/assets/js/mathjax-config.js" defer> </script>
<script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script> $(function(){$("#includedContent").load("./bibtex.html");}); </script>
<meta name="description" content="Assisting the Adversary to Improve GAN Training">
<script type="text/javascript" src=""></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<section id="sec-title-slide">
    <h1 class="title">Assisting the Adversary to Improve GAN Training</h1><h2 class="author">Andreas Munk, William Harvey &amp; Frank Wood</h2>
    <h3 class="affiliation">University of British Columbia</h3>
    <h3 class="conference">IJCNN 2021</h3><h4 class="email"><a href="mailto:amunk@cs.ubc.ca">amunk@cs.ubc.ca</a></h4><h4 class="date">July, 2021</h4>
</section>

</section>
<div id="hiddenMathbox" style="display: none;">
<p>
\(
\newcommand{\ie}{i.e.}
\newcommand{\eg}{e.g.}
\newcommand{\etal}{\textit{et~al.}}
\newcommand{\wrt}{w.r.t.}
\newcommand{\bra}[1]{\langle #1 \mid}
\newcommand{\ket}[1]{\mid #1\rangle}
\newcommand{\braket}[2]{\langle #1 \mid #2 \rangle}
\newcommand{\bigbra}[1]{\big\langle #1 \big\mid}
\newcommand{\bigket}[1]{\big\mid #1 \big\rangle}
\newcommand{\bigbraket}[2]{\big\langle #1 \big\mid #2 \big\rangle}
\newcommand{\grad}{\boldsymbol{\nabla}}
\newcommand{\divop}{\grad\scap}
\newcommand{\pp}{\partial}
\newcommand{\ppsqr}{\partial^2}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\trans}[1]{#1^\mr{T}}
\newcommand{\dm}{\,\mathrm{d}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\krondel}[1]{\delta_{#1}}
\newcommand{\limit}[2]{\mathop{\longrightarrow}_{#1 \rightarrow #2}}
\newcommand{\measure}{\mathbb{P}}
\newcommand{\scap}{\!\cdot\!}
\newcommand{\intd}[1]{\int\!\dm#1\: }
\newcommand{\ave}[1]{\left\langle #1 \right\rangle}
\newcommand{\br}[1]{\left\lbrack #1 \right\rbrack}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\tub}[1]{\left\{#1\right\}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\evalat}[1]{\left.#1\right\vert}
\newcommand*{\given}{\mid}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\newcommand{\newterm}[1]{{\bf #1}}
\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\boldsymbol{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}
\def\eps{{\epsilon}}
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}
\def\vzero{{\boldsymbol{0}}}
\def\vone{{\boldsymbol{1}}}
\def\vmu{{\boldsymbol{\mu}}}
\def\vtheta{{\boldsymbol{\theta}}}
\def\va{{\boldsymbol{a}}}
\def\vb{{\boldsymbol{b}}}
\def\vc{{\boldsymbol{c}}}
\def\vd{{\boldsymbol{d}}}
\def\ve{{\boldsymbol{e}}}
\def\vf{{\boldsymbol{f}}}
\def\vg{{\boldsymbol{g}}}
\def\vh{{\boldsymbol{h}}}
\def\vi{{\boldsymbol{i}}}
\def\vj{{\boldsymbol{j}}}
\def\vk{{\boldsymbol{k}}}
\def\vl{{\boldsymbol{l}}}
\def\vm{{\boldsymbol{m}}}
\def\vn{{\boldsymbol{n}}}
\def\vo{{\boldsymbol{o}}}
\def\vp{{\boldsymbol{p}}}
\def\vq{{\boldsymbol{q}}}
\def\vr{{\boldsymbol{r}}}
\def\vs{{\boldsymbol{s}}}
\def\vt{{\boldsymbol{t}}}
\def\vu{{\boldsymbol{u}}}
\def\vv{{\boldsymbol{v}}}
\def\vw{{\boldsymbol{w}}}
\def\vx{{\boldsymbol{x}}}
\def\vy{{\boldsymbol{y}}}
\def\vz{{\boldsymbol{z}}}
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}
\def\mA{{\boldsymbol{A}}}
\def\mB{{\boldsymbol{B}}}
\def\mC{{\boldsymbol{C}}}
\def\mD{{\boldsymbol{D}}}
\def\mE{{\boldsymbol{E}}}
\def\mF{{\boldsymbol{F}}}
\def\mG{{\boldsymbol{G}}}
\def\mH{{\boldsymbol{H}}}
\def\mI{{\boldsymbol{I}}}
\def\mJ{{\boldsymbol{J}}}
\def\mK{{\boldsymbol{K}}}
\def\mL{{\boldsymbol{L}}}
\def\mM{{\boldsymbol{M}}}
\def\mN{{\boldsymbol{N}}}
\def\mO{{\boldsymbol{O}}}
\def\mP{{\boldsymbol{P}}}
\def\mQ{{\boldsymbol{Q}}}
\def\mR{{\boldsymbol{R}}}
\def\mS{{\boldsymbol{S}}}
\def\mT{{\boldsymbol{T}}}
\def\mU{{\boldsymbol{U}}}
\def\mV{{\boldsymbol{V}}}
\def\mW{{\boldsymbol{W}}}
\def\mX{{\boldsymbol{X}}}
\def\mY{{\boldsymbol{Y}}}
\def\mZ{{\boldsymbol{Z}}}
\def\mBeta{{\boldsymbol{\beta}}}
\def\mPhi{{\boldsymbol{\Phi}}}
\def\mLambda{{\boldsymbol{\Lambda}}}
\def\mSigma{{\boldsymbol{\Sigma}}}
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}
\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}
\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}
\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\newcommand{\vxlat}{\vx_{\mr{lat}}}
\newcommand{\vxobs}{\vx_{\mr{obs}}}
\newcommand{\block}[1]{\underbrace{\begin{matrix}1 & \cdots & 1\end{matrix}}_{#1}}
\newcommand{\blockt}[1]{\begin{rcases} \begin{matrix} ~\\ ~\\ ~ \end{matrix} \end{rcases}{#1}}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\)
</p>
</div>

<section>
<section id="slide-org0ba99d4">
<h2 id="org0ba99d4">Closing the gap between theory and practice</h2>
<ul>
<li>The GAN framework is a minimax game, that pivots an adversary
(critic/discriminator) against a generator</li>

<li>The adversary is often assumed optimal in theoretical analysis and in the
construction of different GANs
<ul>
<li>In practice this is essentially never true</li>

</ul></li>

<li>We consider a largely overlooked approach to constraining GAN training
[<a href="#mescheder2017numerics">Mescheder et&nbsp;al., 2017</a>,<a href="#nagarajan2017gradient">Nagarajan and Kolter, 2017</a>]
<ul>
<li>Re-motivated to bridge the gap between theory and practice</li>

</ul></li>

<li>Accounts for the adversary being suboptimal - <i>The Adversarys Assistant</i>
(AdvAs)
<ul>
<li>A regularizer which encourages the generator to move towards points where
the current adversary is optimal</li>

</ul></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>


</section>
</section>
<section>
<section id="slide-orgc3e995d">
<h2 id="orgc3e995d">Generative adversarial networks (GANs) [<a href="#goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>]</h2>
<ul>
<li>A generator is neural network \(g:\gZ\rightarrow\gX\subseteq\real^{D_{x}}\)
which maps from a random vector \(\vz\in\gZ\) to an output \(\vx \in \gX\)
<ul>
<li>\(g\) characterizes a distribution \(p_{\theta}(\vx)\), where
\(\theta\in\Theta\subseteq\mathbb{R}^{D_g}\) denotes the generator&rsquo;s
parameters</li>

</ul></li>

<li data-fragment-index="2" class="fragment appear">Aim is to match \(p_{\theta}\) with some real target distribution
\(p_{\mr{true}}\)</li>

<li data-fragment-index="3" class="fragment appear">An adversary \(a_\phi:\gX\rightarrow \gA\) with parameters
\(\phi\in\Phi\subseteq\real^{D_a}\)</li>

<li data-fragment-index="4" class="fragment appear"><p>
Generally we frame GANs as a minimax game,
</p>

<p>
\[\min_{\theta}\max_{\phi}h(p_{\theta},a_\phi)\]
</p></li>

<li data-fragment-index="5" class="fragment appear">If the minimax game of perfectly solved, then \(p_{\theta} = p_{\mr{true}}\)</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orgbb186d6">
<h2 id="orgbb186d6"><i>The optimal adversary assumption</i> and the minimization of divergences</h2>
<ul>
<li>Before each generator update, the adversary is assumed optimal</li>

<li data-fragment-index="2" class="fragment appear"><p>
When \(a\) is a parameterized by a neural net with parameters \(\phi\in\Phi\), we
define
</p>

<p>
\[\Phi^*(\theta) = \{\phi \in \Phi \mid h(p_\theta, a_\phi) = \max_{a \in
    \gF}h(p_{\theta},a) \}\]
</p>

<p>
where \(\gF\) is the class of permissible adversary functions
</p></li>

<li data-fragment-index="3" class="fragment appear"><p>
This assumption turns the two-player minimax game into a case of minimizing an
objective w.r.t. \(\theta\) alone
</p>

<p>
\[
  \gM(p_{\theta}) = h(p_\theta, a_{\phi^*}) \quad \text{where} \quad \phi^*\in\Phi^*(\theta).
  \]
</p></li>

<li data-fragment-index="4" class="fragment appear">For instance [<a href="#goodfellow2014generative">Goodfellow et&nbsp;al., 2014</a>] showed that \(\gM(p_{\theta}) \propto \text{JSD}(p_{\mr{true}} || p_\theta)\)</li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org651cb66">
<h2 id="org651cb66">Does an optimal adversary lead to optimal gradients?</h2>
<ul>
<li>The goal of training the generator can be seen as minimizing \(\gM(p_{\theta})\)
<ul>
<li>In practice the generator update requires it&rsquo;s gradient \(\grad_\theta
    \gM(p_{\theta})\) (e.g. gradient descent)</li>

</ul></li>

<li data-fragment-index="2" class="fragment appear">We want \(\grad_\theta \gM(p_{\theta})\mid_{\theta=\theta'}\) but calculate in
practice the partial derivative \(D_1 h (p_\theta,
  a_{\phi^*})\mid_{\theta=\theta'}\)
<ul>
<li>Where we defined \(D_1 h (p_\theta, a_\phi)\) to denote the partial derivative
of \(h(p_\theta, a_\phi)\) with respect to \(\theta\) with \(\phi\) kept constant.</li>

</ul></li>

<li data-fragment-index="3" class="fragment appear"><p>
By extending Theorem 1 in [<a href="#milgrom2002envelope">Milgrom and Segal, 2002</a>] we have that
</p>

<blockquote>
<p>
<b>Theorem 1</b>. Let \(\gM(p_{\theta})=h(p_{\theta},a_{\phi^*})\) for any
\(\phi^*\in\Phi^{*}(\theta)\). Assuming that \(\gM(p_{\theta})\) is differentiable
w.r.t. \(\theta\) and \(h(p_{\theta},a_{\phi})\) is differentiable w.r.t. \(\theta\)
for all \(\phi\in\Phi^{*}(\theta)\), then if \(\phi^{*}\in\Phi^{*}(\theta)\) we
have
\[
    \grad_{\theta}\gM(p_{\theta})=D_1 h(p_{\theta},a_{\phi^{*}})
    \]
</p>
</blockquote></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orge101406">
<h2 id="orge101406">Adversary constructors</h2>
<ul>
<li>An adversary constructor is a function \(f:\Theta\rightarrow\Phi\), where for
all \(\theta \in \Theta\), \(f(\theta) = \phi^*\in\Phi^{*}(\theta)\),</li>

<li data-fragment-index="2" class="fragment appear"><p>
We cannot compute the function \(f(\theta)\) in general. However, they
lead to a condition which must be satisfied for Theorem 1 to be invoked
</p>

<blockquote>
<p>
<b>Corollary 1</b>. Let \(f:\Theta\rightarrow\Phi\) be a differentiable mapping such
  that for all \(\theta \in \Theta\),
  \(\gM(p_{\theta})=h(p_{\theta},a_{f(\theta)})\). If the conditions in Theorem 1
  are satisfied and the Jacobian matrix of \(f\) with respect to \(\theta\),
  \(\mJ_{\theta}(f)\) exists for all \(\theta\in\Theta\) then
</p>
<div>
\begin{equation}\label{eq:cor}
  \trans{D_2 h(p_{\theta},a_{f(\theta)})} \mJ_{\theta}(f) = 0.
\end{equation}

</div>
</blockquote></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org5b12788">
<h2 id="org5b12788">Assisting the Adversary - introducing AdvAs</h2>
<ul>
<li>Eq. \ref{eq:cor} is a necessary condition for invoking Theorem 1
<ul>
<li>But cannot be computed</li>

</ul></li>
<li data-fragment-index="2" class="fragment appear">Can compute \(D_2 h(p_{\theta},a_{\phi})\)
<ul>
<li>Use the magnitude as an approximate measure of how &ldquo;close&rdquo; we are to
having an optimal adversary</li>

</ul></li>

<li data-fragment-index="3" class="fragment appear"><p>
Consider the following loss functions
</p>

<div>
\begin{align}
L_{\text{gen}}(\theta, \phi) &= h(p_{\theta},a_\phi) \nonumber \\
L_{\text{adv}}(\theta, \phi) &= -h(p_{\theta},a_\phi) \nonumber
\end{align}

</div></li>

<li data-fragment-index="4" class="fragment appear"><p>
Using AdvAs implies
</p>

<p>
\[
 L^{\mr{AdvAs}}_{\mr{gen}}(\theta,\phi) = L_{\mr{gen}}(\theta,\phi) + \lambda\cdot r(\theta,\phi)
 \]
</p>

<p>
with \(r(\theta,\phi) = \norm{D_{2} {L_{\mr{adv}}}(\theta,\phi)}_2^2\) and \(\lambda\) being a hyperparameter.
Also found in [<a href="#mescheder2017numerics">Mescheder et&nbsp;al., 2017</a>,<a href="#nagarajan2017gradient">Nagarajan and Kolter, 2017</a>]</p></li>

</ul>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-org7b509ab">
<h2 id="org7b509ab">Experiments</h2>
</section>
</section>
<section>
<section id="slide-orgfa9182d">
<h2 id="orgfa9182d">CelebA and StyleGan [<a href="#karras2020analyzing">Karras et&nbsp;al., 2020</a>]</h2>

<div id="orgfa50159" class="figure">
<p><object type="image/svg+xml" data="../../../assets/img/advas/figs/graph-and-images.svg" class="org-svg" width="1000">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 1: </span><b>Bottom:</b> FID scores throughout training estimated with \(1000\) samples, plotted against number of epochs (left) and training time (right). FID scores for AdvAs decrease more on each iteration at the start of training and converge to be \(7.5\%\) lower. <b>Top:</b> The left two columns show uncurated samples with and without AdvAs after 2 epochs. The rightmost two columns show uncurated samples from networks at the end of training. In each grid of images, each row is generated by a network with a different training seed and shows 3 images generated by passing a different random vector through this network. AdvAs leads to obvious qualitative improvement early in training.</p>
</div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>

</section>
</section>
<section>
<section id="slide-orgd5c2262">
<h2 id="orgd5c2262">References</h2>
<div id="includedContent"></div>
<div class="slide-footer">Andreas Munk, <a href="mailto:andreas@ammunk.com">amunk@cs.ubc.ca</a>, <a href="https://ammunk.com">ammunk.com</a></div>
</section>
</section>
</div>
</div>
<script src="https://ammunk.com/reveal.js/dist/reveal.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/highlight/highlight.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/search/search.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/zoom/zoom.js"></script>
<script src="https://ammunk.com/reveal.js/plugin/notes/notes.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: true,
center: true,
slideNumber: 't',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,

overview: true,
width: 2000,
height: 1400,
margin: 0.00,
minScale: 0.30,
maxScale: 2.50,

transition: 'linear',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealHighlight, RevealSearch, RevealZoom, RevealNotes ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
