---
title: The Evidence Lower Bound (ELBO) and Variational Bayes
layout: post
date: 2019-11-17 18:41:00-0400
comments: true
---

{% include mathjax-macros.html %}

<div id="outline-container-orgb974914" class="outline-2">
<h2 id="orgb974914">Introduction</h2>
<div class="outline-text-2" id="text-orgb974914">
<p>
This note aims at deriving the <i>evidence lower bound</i> (ELBO) and motivate it
using a Bayesian inference perspective. Here we consider an observed variable
\(y\) and seek to infer the distribution over latent variables \(z\) given \(y\). That
is to say, we want to find the posterior distribution over the latent variables,
</p>

<p>
\[
 p(x|y) = \frac{p(y,x)}{p(y)} = \frac{p(y,x)}{\int p(y|x)p(x)\dm x}.
\]
</p>

<p>
In general we cannot compute this distribution, and so we need to turn towards
other methods like Markov Chain Monte Carlo, Importance Sampling, <i>Variational
Bayes</i> (VB), etc. 
</p>

<p>
It is within VB that we find the ELBO is an essential mathematical expression
that allows for approximating the posterior distribution \(q(x|y)\approx p(x|y)\).
It has been used successfully in many applications and machine learning methods
such as Variational autoencoders (VAEs) [<a href="#kingma2014autoencoding">Kingma and Welling, 2014</a>]. For a more in-depth
explanation of VB and also the ELBO I suggest looking into excellent the pattern
recognition book by Bishop [<a href="#bishop2006a">Bishop, 2006</a>] and the review paper on variational
inference [<a href="#blei2017a">Blei et&nbsp;al., 2017</a>].
</p>
</div>
</div>

<div id="outline-container-org61eb6b5" class="outline-2">
<h2 id="org61eb6b5">The ELBO</h2>
<div class="outline-text-2" id="text-org61eb6b5">
<p>
We will derive the ELBO from an inference perspective, i.e. we want to
approximate \(p(x|y)\) with \(q(x|y;\theta)\). Here we explicitly parameterized \(q\)
with \(\theta\) and we shall assume the flexibility of \(q\) is precisely defined by
\(\theta\). The objective then is to find \(\theta\) such that \(q(x|y;\theta)\approx
p(x|y)\). This requires some mathematical expression which tells us how to find
such a \(\theta\) and the ELBO is one such expression. We denote the ELBO as \(\gL\)
and as we will see it takes the following form,
</p>

\begin{equation}
\label{eq:elbo}
\gL = \E_{q(x|y;\theta)}\br{\ln{\frac{p(x,y)}{q(x|y;\theta)}}},
\end{equation}

<p>
and we shall see two different ways to derive that expression.
</p>
</div>

<div id="outline-container-orgd2d986e" class="outline-3">
<h3 id="orgd2d986e">From the KL divergence</h3>
<div class="outline-text-3" id="text-orgd2d986e">
<p>
To measure closeness between \(q\) and \(p\) and consider the
Kullback&#x2013;Leibler (KL) divergence,
</p>

<p>
$$
</p>
\begin{align*}
  \mr{KL}\br{q(x|y;\theta)|p(x|y)} &= -\int \ln{\frac{p(x|y)}{q(x|y;\theta)}} q(x|y;\theta)\dm x\nn\\
& = -\br{ \int \ln{\frac{p(x,y)}{q(x|y;\theta)}} q(x|y;\theta)\dm x  - \int \ln p(y) q(x|y;\theta)\dm x } \nn\\
& = -\E_{q(x|y;\theta)}\br{\ln{\frac{p(x,y)}{q(x|y;\theta)}}} \dm x  + \ln p(y),\nn
\end{align*}
<p>
$$
</p>

<p>
which we rearrange to,
</p>

<p>
\[
  \ln p(y) = \mr{KL}\br{q(x|y;\theta)|p(x|y)} + \E_{q(x|y;\theta)}\br{\ln{\frac{p(x,y)}{q(x|y;\theta)}}} = \mr{KL}\br{q(x|y)|p_{\theta}(x|y)} + \gL.
\]
</p>

<p>
Because the KL-divergence is non-negative we identify \(\ln p(y)\ge\gL\), making
\(\gL\) a lower bound on the evidence \(p(y)\), hence the name.
</p>
</div>
</div>

<div id="outline-container-org681ef31" class="outline-3">
<h3 id="org681ef31">From maximizing the model evidence</h3>
<div class="outline-text-3" id="text-org681ef31">
<p>
As shown above there is a direct connection between the evidence \(p(y)\) and the
ELBO \(\gL\) and we will here derive the expression of \(\gL\) directly from \(\ln
p(y)\) using <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen&rsquo;s inequality</a>,
</p>

<p>
\[
f(\E\br{x}) \le \E\br{f(x)}, \quad \text{for a convex function}~f,
\]
or
\[
f(\E\br{x}) \ge \E\br{f(x)}, \quad \text{for a concave function}~f.
\]
</p>

<p>
With this we can write,
</p>

<p>
\[
\balnn
\ln p(y) &= \ln \int p(y|x)p(x) \dm x \nn\\
&= \ln \int \frac{p(y,x)}{q(x|y;\theta)}q(x|y;\theta) \dm x \nn\\
&\ge \E_{q(x|y;\theta)}\br{\ln\frac{p(y,x)}{q(x|y;\theta)}},\nn
\ealnn
\]
which is exactly the expression in \eqref{elbo}
</p>
</div>
</div>
</div>


<div id="outline-container-orgbf2d2f4" class="outline-2">
<h2 id="orgbf2d2f4">Related topics</h2>
<div class="outline-text-2" id="text-orgbf2d2f4">
<p>
The topics of VB and the ELBO, relates to a series of other
topics some of which I will be making other notes about. Some of these topics 
include but is certainly not limited:
</p>

<ul class="org-ul">
<li>The <i>expectation maximization</i> (EM) algorithm</li>
<li>VAEs</li>
<li>Inference compilation [<a href="#le2016inference">Le et&nbsp;al., 2017</a>]</li>
</ul>

<p>
And much, much more.
</p>

<div id="bibliography">
<h2>References</h2>

<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bishop2006a">Bishop, 2006</a>]
</td>
<td class="bibtexitem">
Bishop, C.&nbsp;M. (2006).
 <em>Pattern Recognition and Machine Learning</em>.
 Springer,.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="blei2017a">Blei et&nbsp;al., 2017</a>]
</td>
<td class="bibtexitem">
Blei, D.&nbsp;M., Kucukelbir, A., and McAuliffe, J.&nbsp;D. (2017).
 Variational inference: A review for statisticians.
 <em>Journal of the American Statistical Association</em>,
  112(518):859--877.
[&nbsp;<a href="http://dx.doi.org/10.1080/01621459.2017.1285773">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kingma2014autoencoding">Kingma and Welling, 2014</a>]
</td>
<td class="bibtexitem">
Kingma, D.&nbsp;P. and Welling, M. (2014).
 Auto-encoding variational bayes.
 <em>arXiv:1312.6114 [cs, stat]</em>.
[&nbsp;<a href="http://arxiv.org/abs/1312.6114">arXiv</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="le2016inference">Le et&nbsp;al., 2017</a>]
</td>
<td class="bibtexitem">
Le, T.&nbsp;A., Baydin, A.&nbsp;G., and Wood, F. (2017).
 Inference compilation and universal probabilistic programming.
 In <em>Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics</em>, volume&nbsp;54 of <em>Proceedings of
  Machine Learning Research</em>, pages 1338--1348, Fort Lauderdale, FL, USA. PMLR.
[&nbsp;<a href="http://arxiv.org/abs/1610.09900">arXiv</a>&nbsp;]

</td>
</tr>
</table>
</div>
</div>
</div>
