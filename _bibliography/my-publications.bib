---
---

@incollection{baydin2019efficient,
  title = {Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Baydin, Atilim Gunes and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Naderiparizi, Saeid and Munk, Andreas and Liu, Jialin and {Gram-Hansen}, Bradley and Louppe, Gilles and Meadows, Lawrence and Torr, Philip and Lee, Victor and Cranmer, Kyle and Prabhat, Mr. and Wood, Frank},
  year = {2019},
  pages = {5459--5472},
  publisher = {Curran Associates, Inc.},
  copyright = {All rights reserved},
  selected = {true},
  abbr = {NeurIPS}
}

@inproceedings{baydin2019etalumis,
  title = {Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale},
  shorttitle = {Etalumis},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  author = {Baydin, Atilim G{\"u}ne{\c s} and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Meadows, Lawrence and Liu, Jialin and Munk, Andreas and Naderiparizi, Saeid and {Gram-Hansen}, Bradley and Louppe, Gilles and Ma, Mingfei and Zhao, Xiaohui and Torr, Philip and Lee, Victor and Cranmer, Kyle and Prabhat and Wood, Frank},
  year = {2019},
  month = nov,
  pages = {1--24},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3295500.3356180},
  abstract = {Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN-LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global mini-batch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.},
  copyright = {All rights reserved},
  isbn = {978-1-4503-6229-0},
  keywords = {deep learning,inference,probabilistic programming,simulation},
  series = {SC '19}
}

@article{gram-hansen2019efficient,
  title = {Efficient Bayesian Inference for Nested Simulators},
  author = {{Gram-Hansen}, Bradley and de Witt, Christian Schroeder and Zinkov, Robert and Naderiparizi, Saeid and Scibior, Adam and Munk, Andreas and Wood, Frank and Ghadiri, Mehrdad and Torr, Philip and Teh, Yee Whye and Baydin, Atilim Gunes and Rainforth, Tom},
  year = {2019},
  month = oct,
  abstract = {We introduce two approaches for efficient and scalable inference in stochastic simulators for which the density cannot be evaluated directly due to, for example, rejection sampling loops.},
  copyright = {All rights reserved},
  language = {en}
}

@article{harvey2019attention,
  title = {Attention for Inference Compilation},
  author = {Harvey, William and Munk, Andreas and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Bergholm, Alexander and Wood, Frank},
  year = {2019},
  month = oct,
  abstract = {We present a new approach to automatic amortized inference in universal probabilistic programs which improves performance compared to current methods. Our approach is a variation of inference compilation (IC) which leverages deep neural networks to approximate a posterior distribution over latent variables in a probabilistic program. A challenge with existing IC network architectures is that they can fail to model long-range dependencies between latent variables. To address this, we introduce an attention mechanism that attends to the most salient variables previously sampled in the execution of a probabilistic program. We demonstrate that the addition of attention allows the proposal distributions to better match the true posterior, enhancing inference about latent variables in simulators.},
  archivePrefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1910.11961},
  eprinttype = {arxiv},
  journal = {arXiv:1910.11961 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{munk2018semisupervised,
  title = {Semi-Supervised Sleep-Stage Scoring Based on Single Channel EEG},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author = {Munk, A. M. and Olesen, K. V. and Gangstad, S. W. and Hansen, L. K.},
  year = {2018},
  month = apr,
  pages = {2551--2555},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461982},
  abstract = {The field of automatic sleep stage classification based on EEG has enjoyed substantial attention during the last decade, which has resulted in several supervised classification algorithms with highly encouraging performance. Such supervised machine learning algorithms require large training sets that have been manually labelled, and are time- and resource-consuming to acquire. Here we present a semi-supervised approach that can learn to distinguish the sleep stages from a one-night data set where only a fraction has been manually labelled. We show that for fractions larger than 50\%, our semi-supervised approach performs as good as a similar, fully-supervised model.},
  copyright = {All rights reserved},
  keywords = {automatic sleep stage classification,Brain modeling,EEG,electroencephalography,Electroencephalography,fully-supervised model,Gaussian mixture model,generalizable Gaussian mixture model,learning (artificial intelligence),medical signal processing,non-negative matrix factorizaton,pattern classification,semi-supervised learning,semisupervised approach performs,semisupervised sleep-stage scoring,signal classification,sleep,Sleep,sleep stage scoring,sleep stages,Spectrogram,supervised classification algorithms,supervised machine learning algorithms,Training}
}

@article{munk2019deep,
  title = {Deep Probabilistic Surrogate Networks for Universal Simulator Approximation},
  author = {Munk, Andreas and {\'S}cibior, Adam and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Stewart, Andrew and Fernlund, Goran and Poursartip, Anoush and Wood, Frank},
  year = {2019},
  month = oct,
  abstract = {We present a framework for automatically structuring and training fast, approximate, deep neural surrogates of existing stochastic simulators. Unlike traditional approaches to surrogate modeling, our surrogates retain the interpretable structure of the reference simulators. The particular way we achieve this allows us to replace the reference simulator with the surrogate when undertaking amortized inference in the probabilistic programming sense. The fidelity and speed of our surrogates allow for not only faster "forward" stochastic simulation but also for accurate and substantially faster inference. We support these claims via experiments that involve a commercial composite-materials curing simulator. Employing our surrogate modeling technique makes inference an order of magnitude faster, opening up the possibility of doing simulator-based, non-invasive, just-in-time parts quality testing; in this case inferring safety-critical latent internal temperature profiles of composite materials undergoing curing from surface temperature profile measurements.},
  archivePrefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1910.11950},
  eprinttype = {arxiv},
  journal = {arXiv:1910.11950 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  selected = {true}
}

@article{munk2020assisting,
  title = {Assisting the Adversary to Improve GAN Training},
  author = {Munk, Andreas and Harvey, William and Wood, Frank},
  year = {2020},
  month = oct,
  abstract = {We propose a method for improved training of generative adversarial networks (GANs). Some of the most popular methods for improving the stability and performance of GANs involve constraining or regularizing the discriminator. Our method, on the other hand, involves regularizing the generator. It can be used alongside existing approaches to GAN training and is simple and straightforward to implement. Our method is motivated by a common mismatch between theoretical analysis and practice: analysis often assumes that the discriminator reaches its optimum on each iteration. In practice, this is essentially never true, often leading to poor gradient estimates for the generator. To address this, we introduce the Adversary's Assistant (AdvAs). It is a theoretically motivated penalty imposed on the generator based on the norm of the gradients used to train the discriminator. This encourages the generator to move towards points where the discriminator is optimal. We demonstrate the effect of applying AdvAs to several GAN objectives, datasets and network architectures. The results indicate a reduction in the mismatch between theory and practice and that AdvAs can lead to improvement of GAN training, as measured by FID scores.},
  archivePrefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {2010.01274},
  eprinttype = {arxiv},
  journal = {arXiv:2010.01274 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
  selected={true}
}

@article{naderiparizi2019amortized,
  title = {Amortized Rejection Sampling in Universal Probabilistic Programming},
  author = {Naderiparizi, Saeid and {\'S}cibior, Adam and Munk, Andreas and Ghadiri, Mehrdad and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and {Gram-Hansen}, Bradley and {de Witt}, Christian Schroeder and Zinkov, Robert and Torr, Philip H. S. and Rainforth, Tom and Teh, Yee Whye and Wood, Frank},
  year = {2019},
  month = nov,
  abstract = {Existing approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. An instance of this is importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure. In this paper we develop a new and efficient amortized importance sampling estimator. We prove finite variance of our estimator and empirically demonstrate our method's correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework.},
  archivePrefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1910.09056},
  eprinttype = {arxiv},
  journal = {arXiv:1910.09056 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat},
}
